\documentclass[]{article}
\setlength\parindent{0pt}
\usepackage[a4paper,bindingoffset=0.2in,%
left=1in,right=1in,top=1in,bottom=1in,%
footskip=.25in]{geometry}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{pgfplots}
\usepackage{layouts}
\usepackage{float}
\usepackage[skip=0pt]{caption}

%opening
\title{Topological Data Analysis of Treasury Yield Curve Rates}
\author{Christopher Shultz}

\begin{document}

\maketitle

\begin{abstract}
\noindent
We explore the use of Topological Data Analysis (TDA) to study the evolution of US Treasury Yield Curve Rates from 2015 - 2020. We particularly focus on the use of TDA to understand the shape and structure of our multidimensional rates series and whether useful economic inference can be derived from topological tools.

\end{abstract}

\section{Introduction}

% WHAT IS TDA
Topological Data Analysis (TDA) is an emerging multidisciplinary field that uses tools from topology, statistics, and scientific computing to extract insights about complex data. Many existing papers introduce the technique to an unfamiliar reader \cite{bubenik_2015, chazal_2017, munch_2017, wasserman_2018}. From a practical perspective, TDA can be used primarily to explore the structure of multidimensional data with regard to shape and connectivity. \\

% WHERE HAS IT BEEN USED
TDA has shown significant promise in a variety of fields, including cosmology \cite{chen_et_al_2015, van_de_weygaert_et_al_2010, van_de_weygaert_et_al_2011, sousbie_2011}, image analysis \cite{bonis_et_al_2016, li_et_al_2014, carriere_et_al_2015, singh_et_al_2014, adcock_et_al_2014}, finance \cite{gidea_katz_2018, gidea_2016, guo_et_al_2020, majumdar_laha_2020, qiu_et_al_2020, basu_and_dlotko_2020, dlotko_et_al_2019, kim_et_al_2020}, and neuroscience \cite{arai_et_al_2014, babichev_dabaghian_2016, basso_et_al_2016, bendich_et_al_2016}, among many others. This paper does not attempt to present a comprehensive overview of all existing applications of TDA in the scientific literature. However, across all examined papers, the primary theme involves the extraction of insights about the shape and structure of complex multidimensional datasets. \\

%WHAT DO I AIM TO USE IT FOR? 
With this paper, I aim to demonstrate the utility of TDA for exploring economic and financial questions, with a specific focus on treasury yield curve rates. A similar exploration takes place in \cite{gidea_katz_2018} with attention to the equity markets, and we follow their example in our study of rates. Their findings are particularly interesting as they present one of few existing analyses utilizing TDA to explore economic and financial research. Gidea and Katz find a strong rising trend for trading days prior to financial downturns, which implies that TDA may be useful as a new econometric analysis to complement traditional methods. \\

% WHY SHOULD YOU CARE? 
This analysis is meaningful for two main reasons. First, it is (to our knowledge) the first implementation of TDA to examine the shape and structure of treasury yield curve rates. Second, it provides an introduction of TDA to the economics community, who may not have been previously exposed. Based on our survey of the literature, applications of TDA are common in finance, but not much was found in the economics space. Given its utilization across a wide number of fields and the increasing complexity of economic data, TDA has the potential to add significant value to researchers in economics and finance. \\

To complete our analysis, we utilize daily treasury yield curve rates data from 2015 - 2020 across multiple maturity periods. \textbf{\textit{description of method here}. \textit{description of results here}. \textit{description of why this matters here}}. \\

% PAPER OUTLINE
The remainder of this paper is structured as follows: section \ref{tda} describes the mathematical and theoretical details of TDA, including a detailed view of some existing applications; section \ref{yield} provides a summary of the existing literature on yield curve rates and the methodologies that have been used to date. Sections \ref{meth} and \ref{data} explore our methodology and dataset, respectively, providing a detailed outline of the analysis we undertake and the nuances of the underlying dataset. Finally, sections \ref{res} and \ref{conc} present our results, conclusions, and thoughts on future research. \\

\section{Topological Data Analysis}
\label{tda}

% HOW DOES TDA WORK? WHAT IS IT AND WHAT ARE ITS MATH? 
TDA allows for the extraction of new information from high-dimensional, incomplete, and/or complex datasets. It shows promise as an emerging method for providing empirically sound methods for analysis and understanding of the underlying data that can be represented as point clouds in Euclidean space. TDA packages are implemented in a number of languages, including C++, Python, and R. \\ 

Chazel and Michel \cite{chazal_2017} provide a generalized TDA pipeline that most studies implementing TDA employ: 
\begin{enumerate}
	\item The input data is a finite set of points with some distance or similarity metric. In many cases, including this paper, the distance can be induced by the underlying metric space. Typically, data is represented as a series of points in a Euclidean n-dimensional space $\mathbb{E}^n$ \cite{ghrist_2007}. 
	\item Using the point cloud from [1], a "continuous" shape is built on top of the underlying dataset to highlight its shape. This typically takes place through the use of simplicial complexes. 
	\item Topological or geometric information is inferred from the structures built on the data. 
	\item The extracted information provides new families of features and descriptions of the data that can be used to better understand the data or be combined with other existing features for modeling.
\end{enumerate}

% What is a metric space and why does it matter for TDA? 

\subsection{Persistence Diagrams} 

The persistence diagram can show a great deal of information about a given point cloud as well as describing more complicated structure, loops, and voids that are not visible with other methods \cite{munch_2017}. Homology in degree 0 describes the connectedness of the data; degree 1 detects holes or tunnels; degree 2 captures voids, etc.  \cite{bubenik_2015}. Features that \textit{persist} when the resolution changes are referred to as \textbf{persistent homology}. \\

% What is a persistence diagram? 

\subsection{Simplicial Complexes} 

With our collection of data in $\mathbb{E}$, we use the point cloud as the vertices of a combinatorial graph whose edges are determined by proximity. While this approach allows for useful clustering, a number of additional higher order features are ignored under this structure. Additional features can be accurately derived by thinking of the graph as a scaffold for a higher-dimensional object. Specifically, one completes the graph to a simplicial complex - a space built from simple pieces (simplicies) identified combinatorially along faces. 

The next task is to build a useful simplicial complex that represents the structure of the data and which uses the original data as the vertex set. The Vietoris-Rips complex for parameter $t$ is constructed as follows: the vertex set is given by the data itself; For each pair of points $x,y$ in the dataset, we include the edge $xy$ if the distance between them is at most $t:d(x,y)\leq t$. For a higher dimensional simplex given by vertices $x_0, \cdots x_d$, we include the simplex if the complex has all possible edges; explicitly, this means that every vertex $x_0, \cdots x_d$ is within distance $t$ of every other vertex in the simplex. 

\textbf{Definition 1}: Given a collection of points ${x_\alpha} \in \mathbb{E}^n$, the \textbf{Rips Complex}, $\mathcal{R_\epsilon}$ is the abstract simplicial complex whose k-simplices correspond to unordered $(k+1)$-tuples of points ${x_\alpha}$ $k \choose 0$, which are pairwise within distance $\epsilon$. 

The Rips complex is particularly useful for seeing structure in the data as long as the connectivity parameter $t$ is chosen well. The best way to choose $t$ is to look at the continuum of possible $t$ values and explore what appears in terms of structure. 

\subsection{Barcodes} 

After conversion of our data set into a family of simplicial complexes, we view these topological objects via a theory of persistent homology that is encoded in the form of a Betti number: a \textbf{barcode} \cite{ghrist_2007}. 

\section{Treasury Yield Curve Rates} 
\label{yield}

% WHAT DO WE NEED TO SHARE ABOUT TREASURY CURVE YIELD RATES? 
% WHAT ARE THEY AND WHAT DO THEY MEAN

% WHAT PREVIOUS LITERATURE EXPLORES TCYRS? 
% WHAT DID THEY FIND AND WHAT KINDS OF METHODS DID THEY USE? 

% HAS TDA EVER BEEN USED? 

\section{Methodology} 
\label{meth}

% DETAILS OF THE METHODOLOGY / PIPELINE USED IN THIS PAPER

% WHAT SHOULD THIS HELP US DO? 

\section{Data}
\label{data}

% DESCRIPTIVE STATS
The dataset analyzed in this study was retrieved from the US Treasury \footnote{https://www.treasury.gov/resource-center/data-chart-center/interest-rates} and contains daily yield curve rates data from January 2, 2015 to December 31, 2020 for a number of different maturities: 1 mo, 3 mo, 6 mo, 1 year, 2 yr, 3 yr, 5 yr, 7 yr, 10 yr, 20 yr, and 30 yr. The time series are visualized below in Figure \ref{fig:linePlotRates}
, with our shortest and longest maturities highlighted. 

\begin{figure}[H]
	\centering
	\resizebox{0.5\textwidth}{!}{\input{C:/Users/cshul/Desktop/research/topolRate/data/treasuryRatesVis.pgf}}
	\caption{Treasury Yield Curve Rates 2015-2020}
	\label{fig:linePlotRates}
\end{figure}

% DESCRIPTION OF DATA / SOURCES

% VARIETY OF VISUALS EXPLAINING ITS STRUCTURE

% TIME SERIES FEATURES: AUTOCORRELATION, STATIONARITY, ETC. 

\section{Results}
\label{res}

% DESCRIPTION OF A WALKTHROUGH OF THE PIPELINE PROCESS
% DISCUSS FINDINGS ALONG THE WAY

% SUMMARY OF KEY FINDINGS WITH EVIDENCE

\section{Conclusion}
\label{conc}

% WHAT DID WE LEARN? 

% WHAT ARE THE IMPLICATIONS? 

% WHAT ARE DIRECTIONS FOR FUTURE RESEARCH? 

\clearpage
\bibliographystyle{plain}
\bibliography{refs}

\end{document}
